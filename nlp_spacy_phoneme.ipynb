{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dae637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ffaf8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb6404da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US 0 2 GPE\n",
      "10,000 18 24 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('US needs to earn $10,000.')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed03496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj\n",
      "need VERB ROOT\n",
      "to PART aux\n",
      "say VERB xcomp\n",
      "this PRON dobj\n",
      ": PUNCT punct\n",
      "I PRON nsubj\n",
      "am AUX acl\n",
      "perfect ADJ acomp\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I need to say this: I am perfect.')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "#no entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60df34cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I PRON 13656873538139661788 nsubj X True True\n",
      "need need VERB 9188597074677201817 ROOT xxxx True False\n",
      "to to PART 5595707737748328492 aux xx True True\n",
      "say say VERB 14200088355797579614 xcomp xxx True True\n",
      "this this PRON 15267657372422890137 dobj xxxx True True\n",
      ": : PUNCT 11532473245541075862 punct : False False\n",
      "I I PRON 13656873538139661788 nsubj X True True\n",
      "am be AUX 9188597074677201817 acl xx True True\n",
      "perfect perfect ADJ 10554686591937588953 acomp xxxx True False\n",
      ". . PUNCT 12646065887601541794 punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag, token.dep_,\n",
    "         token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "860cd93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like hamburgers. <-> I like salty fries. 0.799431321241256\n",
      "I like hamburgers. <-> The weather is nice today. 0.3878676973488825\n",
      "hamburger\n",
      "hamburger <-> salty frie 0.6337178764122791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/_wjv3dhs0_ngld9lp8505yrr0000gn/T/ipykernel_1003/3525541566.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1, '<->', doc2, doc1.similarity(doc2))\n",
      "/var/folders/qw/_wjv3dhs0_ngld9lp8505yrr0000gn/T/ipykernel_1003/3525541566.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1, '<->', doc3, doc1.similarity(doc3))\n",
      "/var/folders/qw/_wjv3dhs0_ngld9lp8505yrr0000gn/T/ipykernel_1003/3525541566.py:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(word1, '<->', word2, nlp(word1).similarity(nlp(word2)))\n"
     ]
    }
   ],
   "source": [
    "text1 = 'I like hamburgers.'\n",
    "text2 = 'I like salty fries.'\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "doc3 = nlp('The weather is nice today.')\n",
    "print(doc1, '<->', doc2, doc1.similarity(doc2))\n",
    "print(doc1, '<->', doc3, doc1.similarity(doc3))\n",
    "word1 = text1[7:16]\n",
    "print(word1)\n",
    "word2 = text2[7:17]\n",
    "print(word1, '<->', word2, nlp(word1).similarity(nlp(word2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d7fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two   0\n",
      "Apples   0\n",
      "CPU times: user 177 ms, sys: 4.8 ms, total: 182 ms\n",
      "Wall time: 182 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.blank('en')\n",
    "ruler = nlp.add_pipe('attribute_ruler')\n",
    "nlp.analyze_pipes()\n",
    "\n",
    "# patterns = [[{'TAG': 'VB'}]]\n",
    "# attrs = {'POS': 'VERB'}\n",
    "# ruler.add(patterns=patterns, attrs=attrs)\n",
    "\n",
    "# patterns = [\n",
    "#     {\n",
    "#     'patterns': [[{'TAG': 'VB'}]], 'attrs': {'POS': 'VERB'}\n",
    "#     },\n",
    "#     {\n",
    "#     'patterns': [[{'LOWER': 'two'}, {'LOWER': 'apples'}]], \n",
    "#     'attrs': {'LEMMA': 'apple'},\n",
    "#     'index': -1\n",
    "#     },\n",
    "# ]\n",
    "# ruler.add_patterns(patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af80667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 276 ms, sys: 25.6 ms, total: 302 ms\n",
      "Wall time: 305 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'sentencizer': []},\n",
       " 'attrs': {'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['sentencizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.blank('en')\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61308a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In phonology and linguistics, a phoneme /ˈfoʊniːm/ is a unit of sound that can distinguish one word from another in a particular language.\n",
      "\n",
      "For example, in most dialects of English, with the notable exception of the West Midlands and the north-west of England,[1] the sound patterns /sɪn/ (sin) and /sɪŋ/ (sing) are two separate words that are distinguished by the substitution of one phoneme, /n/, for another phoneme, /ŋ/. Two words like this that differ in meaning through the contrast of a single phoneme form a minimal pair. If, in another language, any two sequences differing only by pronunciation of the final sounds [n] or [ŋ] are perceived as being the same in meaning, then these two sounds are interpreted as phonetic variants of a single phoneme in that language.\n",
      "\n",
      "Phonemes that are established by the use of minimal pairs, such as tap vs tab or pat vs bat, are written between slashes: /p/, /b/. To show pronunciation, linguists use square brackets: [pʰ] (indicating an aspirated p in pat).\n",
      "\n",
      "There are differing views as to exactly what phonemes are and how a given language should be analyzed in phonemic (or phonematic) terms. However, a phoneme is generally regarded as an abstraction of a set (or equivalence class) of speech sounds (phones) that are perceived as equivalent to each other in a given language. For example, the English k sounds in the words kill and skill are not identical (as described below), but they are distributional variants of a single phoneme /k/. Speech sounds that differ but do not create a meaningful change in the word are known as allophones of the same phoneme. Allophonic variation may be conditioned, in which case a certain phoneme is realized as a certain allophone in particular phonological environments, or it may otherwise be free, and may vary by speaker or by dialect. Therefore, phonemes are often considered to constitute an abstract underlying representation for segments of words, while speech sounds make up the corresponding phonetic realization, or the surface form.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('phoneme.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "doc = nlp(text)\n",
    "print(doc)\n",
    "sent = list(doc.sents)[0:2]\n",
    "# print(sent)\n",
    "sent_s = ''.join([str(item) for item in sent])\n",
    "# print(sent_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69ccb4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English LANGUAGE\n",
      "two CARDINAL\n",
      "one CARDINAL\n",
      "/n/ phoneme\n",
      "Two CARDINAL\n",
      "two CARDINAL\n",
      "two CARDINAL\n",
      "English LANGUAGE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87db60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "ruler = nlp.add_pipe('entity_ruler', before='ner')\n",
    "# nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12c070df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English LANGUAGE\n",
      "West Midlands GPE\n",
      "two CARDINAL\n",
      "one CARDINAL\n",
      "/n/ Phoneme\n",
      "Two CARDINAL\n",
      "two CARDINAL\n",
      "two CARDINAL\n",
      "English LANGUAGE\n"
     ]
    }
   ],
   "source": [
    "# ner = nlp.add_pipe('ner') \n",
    "# processed = ner(doc) \n",
    "# ner.add_label('phoneme') \n",
    "\n",
    "patterns = [\n",
    "    {\n",
    "    'label': 'Phoneme', 'pattern': '/n/'\n",
    "    },\n",
    "    {\n",
    "    'label': 'GPE', 'pattern': 'West Midlands'   \n",
    "    }\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc_p = nlp(text)\n",
    "for ent in doc_p.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5749c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'things,', \"way's\", 'the', 'alreaday', 'with', 'I', 'best', 'energy', 'have', \"don't\", \"'ve\", \"n't\", 'let', \"they've\", \"'s\", 'deal', 'past', 'to', 'n‘t', 'passed,', 'it', 'go.'}\n",
      "{'things,', \"way's\", 'the', 'alreaday', 'with', 'I', 'best', 'energy', 'have', \"don't\", \"'ve\", \"n't\", '‘ve', 'let', \"they've\", \"'s\", 'deal', 'past', 'to', 'n‘t', 'passed,', 'it', 'go.'}\n",
      "{'things,', \"way's\", 'the', 'alreaday', 'with', '‘s', 'I', 'best', 'energy', 'have', \"don't\", \"'ve\", \"n't\", '‘ve', 'let', \"they've\", \"'s\", 'deal', 'past', 'to', 'n‘t', 'passed,', 'it', 'go.'}\n",
      "{'things,', \"way's\", 'n’t', 'the', 'alreaday', 'with', '‘s', 'I', 'best', 'energy', 'have', \"don't\", \"'ve\", \"n't\", '‘ve', 'let', \"they've\", \"'s\", 'deal', 'past', 'to', 'n‘t', 'passed,', 'it', 'go.'}\n",
      "{'things,', \"way's\", 'n’t', 'the', 'alreaday', 'with', '‘s', 'I', 'best', 'energy', 'have', \"don't\", \"'ve\", \"n't\", '’ve', '‘ve', 'let', \"they've\", \"'s\", 'deal', 'past', 'to', 'n‘t', 'passed,', 'it', 'go.'}\n",
      "{'things,', \"way's\", 'n’t', 'the', 'alreaday', 'with', '‘s', 'I', 'best', 'energy', 'have', \"don't\", \"'ve\", \"n't\", '’ve', '‘ve', 'let', \"they've\", \"'s\", 'deal', 'past', 'to', 'n‘t', 'passed,', '’s', 'it', 'go.'}\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = set(\n",
    "    \"\"\"\n",
    "    I don't have the energy to deal with the past things,\n",
    "    they've alreaday passed, the best way's to let it go.\n",
    "    \n",
    "    \"\"\".split()\n",
    ")\n",
    "\n",
    "contractions = [\"n't\", \"'ve\", \"'s\"]\n",
    "STOP_WORDS.update(contractions)\n",
    "\n",
    "for apostrophe in [\"‘\", \"’\"]:\n",
    "# curly quotation marks: alt + ], shift + alt + [\n",
    "    for stopwords in contractions:\n",
    "        STOP_WORDS.add(stopwords.replace(\"'\", apostrophe))\n",
    "        print(STOP_WORDS)\n",
    "        \n",
    "# STOP_WORDS = ['/']\n",
    "# for words in STOP_WORDS:\n",
    "#         words.add(words.replace(\"/\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ee8feb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3004906285683798724, 0, 2), (3004906285683798724, 3, 5)]\n",
      "(3004906285683798724, 0, 2) HELLO WORLD\n",
      "(3004906285683798724, 3, 5) Google maps\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "patterns = [\n",
    "    [{'LOWER': 'hello'}, {'LOWER': 'world'}],\n",
    "    [{'ORTH': 'Google'}, {'ORTH': 'maps'}]\n",
    "]\n",
    "matcher.add('TEST_PATTERNS', patterns, greedy = 'LONGEST')\n",
    "doc = nlp('HELLO WORLD on Google maps.')\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c72860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11010771136823990775, 3, 4)]\n",
      "Email\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('Contact me at aispacevir4812@gmail.com.')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "patterns = [{'LIKE_EMAIL': True}]\n",
    "matcher.add('Email', [patterns])\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "print(nlp.vocab[matches[0][0]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6e878877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "(14677086776663181681, 15, 16) distinguish\n",
      "(14677086776663181681, 69, 70) distinguished\n",
      "(14677086776663181681, 89, 90) differ\n",
      "(14677086776663181681, 113, 114) differing\n",
      "(14677086776663181681, 129, 130) perceived\n",
      "(14677086776663181681, 142, 143) interpreted\n",
      "(14677086776663181681, 158, 159) established\n",
      "(14677086776663181681, 177, 178) written\n",
      "(14677086776663181681, 181, 182) /p/\n",
      "(14677086776663181681, 185, 186) show\n",
      "(14677086776663181681, 189, 190) use\n",
      "(14677086776663181681, 197, 198) indicating\n",
      "(14677086776663181681, 199, 200) aspirated\n",
      "(14677086776663181681, 207, 208) are\n",
      "(14677086776663181681, 208, 209) differing\n",
      "(14677086776663181681, 219, 220) given\n",
      "(14677086776663181681, 223, 224) analyzed\n",
      "(14677086776663181681, 238, 239) regarded\n",
      "(14677086776663181681, 252, 253) sounds\n",
      "(14677086776663181681, 258, 259) perceived\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# file = pd.read_csv('phoneme.txt')\n",
    "\n",
    "with open('phoneme.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "patterns = [{'POS': 'VERB'}]\n",
    "matcher.add('Verb', [patterns])\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:20]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7a9cc588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(2343386794013751105, 273, 276) English k sounds\n",
      "(2343386794013751105, 301, 303) Speech sounds\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "patterns = [{'POS': 'PROPN', 'OP': '+'}, {'POS': 'VERB'}]\n",
    "matcher.add('Prop_n_v', [patterns], greedy = 'LONGEST')\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "149dc5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In phonology and linguistics, a phoneme /ˈfoʊniːm/ is a unit of sound that can distinguish one word from another in a particular language.\n",
      "\n",
      "For example, in most dialects of English, with the notable exception of the West Midlands and the north-west of England,[1] the sound patterns /sɪn/ (sin) and /sɪŋ/ (sing) are two separate words that are distinguished by the substitution of one phoneme, /n/, for another phoneme, /ŋ/. Two words like this that differ in meaning through the contrast of a single phoneme form a minimal pair. If, in another language, any two sequences differing only by pronunciation of the final sounds [n] or [ŋ] are perceived as being the same in meaning, then these two sounds are interpreted as phonetic variants of a single phoneme in that language.\n",
      "\n",
      "Phonemes that are established by the use of minimal pairs, such as tap vs tab or pat vs bat, are written between slashes: /p/, /b/. To show pronunciation, linguists use square brackets: [pʰ] (indicating an aspirated p in pat).\n",
      "\n",
      "There are differing views as to exactly what phonemes are and how a given language should be analyzed in phonemic (or phonematic) terms. However, a phoneme is generally regarded as an abstraction of a set (or equivalence class) of speech sounds (phones) that are perceived as equivalent to each other in a given language. For example, the English k sounds in the words kill and skill are not identical (as described below), but they are distributional variants of a single phoneme /k/. Speech sounds that differ but do not create a meaningful change in the word are known as allophones of the same phoneme. Allophonic variation may be conditioned, in which case a certain phoneme is realized as a certain allophone in particular phonological environments, or it may otherwise be free, and may vary by speaker or by dialect. Therefore, phonemes are often considered to constitute an abstract underlying representation for segments of words, while speech sounds make up the corresponding phonetic realization, or the surface form.\n",
      "\n",
      "7\n",
      "(2919609550747781823, 55, 58) (sin)\n",
      "(2919609550747781823, 60, 63) (sing)\n",
      "(2919609550747781823, 196, 204) (indicating an aspirated p in pat)\n",
      "(2919609550747781823, 226, 230) (or phonematic)\n",
      "(2919609550747781823, 245, 250) (or equivalence class)\n",
      "(2919609550747781823, 253, 256) (phones)\n",
      "(2919609550747781823, 285, 290) (as described below)\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "with open('phoneme.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "print(doc)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "patterns = [{'ORTH': '('},\n",
    "            {'IS_ALPHA': True, 'OP': '*'},\n",
    "            {'IS_PUNCT': True, 'OP': '*'},\n",
    "            {'ORTH': ')'}\n",
    "           ]\n",
    "matcher.add('Explanation', [patterns], greedy = 'LONGEST')\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1013b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(10914080834629532002, 121, 124) [n]\n",
      "(10914080834629532002, 125, 128) [ŋ]\n",
      "(10914080834629532002, 193, 196) [pʰ]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "with open('phoneme.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "# print(doc)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "patterns = [{'TEXT': \"[\"},\n",
    "            {'IS_ALPHA': True, 'OP': '+'},\n",
    "            {'TEXT': ']'}\n",
    "           ]\n",
    "matcher.add('Phoneme', [patterns], greedy = 'LONGEST')\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37541c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
